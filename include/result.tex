The infrastructure implemented for the experiments presented below is open
source and available online at \cite{sources}. The implementation is based on
TensorFlow \cite{abadi2015}, which is an efficient, flexible, and highly
scalable machine-learning library supported by all the major platforms including
the mobile ones.

The experiments are conducted on a \up{GNU}/Linux machine equipped with 16
\up{CPU}s Intel Xeon \up{E5520} 2.27~GHz, 24~\up{GB} of \up{RAM}, and an
\up{HDD}. The machine has no modern \up{GPU}s; therefore, the reported results
have an immerse room for improvement, which concerns not only timing but
potentially accuracy as well due to more extensive training.

\subsection{Data Processing}
Recall that we work with the Google cluster-usage traces \cite{reiss2011}; see
the introduction in \sref{data}. In the experiments, we focus on one particular
resource ($d = 1$), which is the \up{CPU} usage of the tasks run in the cluster.
The resource-usage table contains two apposite columns: the average and maximal
CPU usage over five-minute intervals; we extract the former.

The grouping and indexing steps of the data-processing pipeline described in
\sref{grouping} and \sref{indexing}, respectively, take approximately 60 hours
(sequentially). Most of this time is spent converting \up{CSV} into SQLite,
which can be avoided by working with \up{CSV} directly. However, SQLite provides
a convenient way for working with the data. Moreover, since these operations
have to be done only once, their computational cost can be safely considered
negligible.

\input{include/assets/figures/histogram}
Regarding the selection stage delineated in \sref{selection}, we filter those
traces that contain 5--50 data points; in other words, we let $l_i \in [5, 50]$
in \eref{trace}. It can be seen in \fref{histogram}, which shows a truncated
histogram of the traces' lengths, that such traces constitute around 74\% of the
total number of traces (around 18 out of 24 million). The time taken by the
selection stage depends on the number of traces to be processed. We experiment
with two million random traces, that is, $n = 2 \times 10^6$ in \eref{traces} or
around 11\% of the 5--50 resource-usage traces. Fetching and storing on disk
these many traces take less than three hours. Note that this has to be repeated
only when the selection criteria change, which happens very rarely.

\subsection{Modeling and Prediction}
The variant of gradient descent that we employ for minimizing of the loss
function is Adam \cite{kingma2014}, which is an adaptive optimization algorithm.
The algorithm is utilized with its default settings recommended by the
algorithm's authors.

The hyperparameters that we consider are the number of cells $c$ (blue boxes in
\fref{model}), number of units per cell $u$ (double circles in \fref{model}),
and probability of dropout $p$. Specifically, we let $c \in \{1, 2, 3, 4, 5\}$,
$u \in \{100, 200, 400, 800, 1600\}$, and $p \in \{0.0, 0.1\}$, which yields 50
different combinations in total. The combinations are explored using the
Hyperband algorithm \cite{li2016}, introduced in \sref{validation}, with its
default settings.

During the exploration, we run up to 8 learning sessions in parallel, which
typically keeps all 16 \up{CPU}s busy. It should be noted that, due to the fact
that the training, validation, and testing data have been cached on disk as a
result of our data-processing pipeline described in \sref{data}, individual
learning sessions do not have any overhead in this regard.

In order to have a baseline for the accuracy of resource-usage prediction, we
use a model based on random walk \cite{hastie2009}, which we shall refer to as
the reference. This model assumes that the next value is equal to the current
one plus a optional random offset, which we set to zero for simplicity.

\input{include/assets/figures/validation}
The validation results can be seen in \fref{validation}.

\input{include/assets/figures/testing}
Finally, the best trained model as identified at the validation stage undergoes
testing using the testing set $X_3$. The corresponding results can be seen in
\fref{testing}.
