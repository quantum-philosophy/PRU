Before we proceed to the actual data processing, let us first give a brief
introduction to the data set we consider, which is the Google cluster-usage
traces \cite{reiss2011}. The traces were collected over 29 days in May 2011.

The resource-usage table of the tasks is provided in the form of 500 archives.
Each archive contains a single \up{CSV} file, which, in turn, contains
performance measurements over a certain time window. Such a format is rather in
convenient given the way the data are supposed to be queried at the learning
stage. For instance, in order to find all the data points that belong to a
particular task, one has to search, in principle, in all the archives. Such a
querying strategy is not practical as reading data is a core operation, which
has to be undertaken millions of times during a learning session. The situation
is exacerbated even further when multiple learning sessions are to be performed,
which is the most common scenario in practice. All in all, an efficient
data-processing pipeline is needed, which is what we shall develop in this
section.

\subsection{Grouping}
The \up{CSV} data from the 500 archives are distributed into separate \up{CSV}
files so that each such file contains all the data points that belong to a
particular job. As a result, there are as many \up{CSV} files as there are jobs.

\subsection{Indexing}
She sells seashells on the seashore.

\subsection{Selection}
\input{include/assets/figures/histogram}
The histogram of the resource-usage traces' lengths can be seen in
\fref{histogram}; the histogram has been truncated on the right-hand side in
order to show the bulk of the traces better. The traces that contain between 5
and 50 data points constitute around 74\% of the data.
